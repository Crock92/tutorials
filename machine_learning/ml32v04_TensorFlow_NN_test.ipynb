{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import sklearn.metrics\n",
    "from IPython.display import Image\n",
    "from addutils import css_notebook\n",
    "import time\n",
    "import json\n",
    "css_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import bokeh.plotting as bk\n",
    "bk.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 TensorFlow Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this serie of notebooks we would like to compare three different libraries for Neural Netwok using a regression problem on temporal data. The problem is to predict the value of the next event using a fully connected feed forward network. Each notebook performs the fitting and prediction of the data using a different library, varying batch size and recording both precision (RMSE) and the time needed to fit the data. \n",
    "\n",
    "This notebook uses TensorFlow. Please refer to previous notebooks in the serie for installation instructions and how-tos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For this serie of tests we used a dataset with more examples and an increased model complexity. We downloaded it from the *UCI Machine Learning Repository*. Follow this link and download the [Individual household electric power consumption](http://archive.ics.uci.edu/ml/datasets.html?format=&task=reg&att=&area=&numAtt=&numIns=&type=&sort=instDown&view=table) dataset. It has more than two milions examples. Please download it in the current directory. It refers to measurements of electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and some sub-metering values are available. In our experiment we use *Voltage* as reference quantity.\n",
    "\n",
    "Settings for this experiments are similar to the one used for previous notebooks. It is possible to choose the number of features (i.e. the number of inputs) and the number of future steps to predict. The variable `percentage` refers to the number of training examples with respect to the whole dataset. For example if you specify 0.7 it means that 70% of the examples will be used for training. The remaining 30% examples are split equally between test set and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 100\n",
    "steps_forward = 1\n",
    "percentage = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = pd.read_csv('household_power_consumption.txt', sep=';')\n",
    "a = x['Voltage'].values\n",
    "a = a[a != '?']\n",
    "a = a.astype(np.float32)\n",
    "\n",
    "vec_size = n_inputs + steps_forward - 1\n",
    "\n",
    "X = np.zeros((a.size-vec_size,n_inputs))\n",
    "\n",
    "y = np.zeros((a.size-vec_size,1))\n",
    "for r in range(a.size-vec_size):\n",
    "    X[r,:] = a[r:r+n_inputs]\n",
    "    y[r,:] = a[r+vec_size]\n",
    "\n",
    "split = int(a.size*percentage) - n_inputs\n",
    "X_train = X[:split]\n",
    "X_test = X[split+((X.shape[0]-split)/2):] \n",
    "X_valid = X[split:split+((X.shape[0]-split)/2)] \n",
    "y_train = y[:split]\n",
    "y_test = y[split+((X.shape[0]-split)/2):]\n",
    "y_valid = y[split:split+((X.shape[0]-split)/2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('X_train shape: %d, %d' % X_train.shape)\n",
    "print('X_test shape: %d, %d' % X_test.shape)\n",
    "print('X_valid shape: %d, %d' % X_valid.shape)\n",
    "print('y_train shape: %d, %d' % y_train.shape)\n",
    "print('y_test shape: %d, %d' % y_test.shape)\n",
    "print('y_valid shape: %d, %d' % y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test varying batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section we perform fitting and prediction of the dataset, using several batch sizes and for each size we record execution time and error. The error measure is Root Mean Squared Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of the Neural Network is composed as follows:\n",
    "\n",
    "* Two hidden layers\n",
    "* First hidden layer with 100 neurons\n",
    "* Second hidden layer with 50 neurons\n",
    "* Dropout with p=0.5 in each layer\n",
    "* Gradient Descent with momentum \n",
    "* Learning Rate = 0.01\n",
    "* Momentum = 0.9\n",
    "* Eearly Stopping\n",
    "* One neuron with linear activation in output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `fit_predict` creates a neural network with the specific architecture and a given batch size and perform fitting and prediction on the dataset. It return the error measure and the time elapsed during fitting.\n",
    "\n",
    "In order to choose which device to use, CPU or GPU, please specify it in the variable `device` in the cell below. To select CPU use the keyword `/cpu:0`, while the keyword `/gpu:0` selects the first GPU. If you have more than one GPU installed on your machine refer to it using the number after the colon, for example the second GPU is `/gpu:1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = '/gpu:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_predict(batch_size, X_train, y_train, X_test, y_test, X_valid, y_valid):\n",
    "    epochs = 100\n",
    "    hidden_size1 = 100\n",
    "    hidden_size2 = 50\n",
    "    p = 0.5\n",
    "\n",
    "    n_inputs = X_train.shape[1]\n",
    "\n",
    "    with tf.device(device):\n",
    "        x = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "        y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "        keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "        W1_x = tf.Variable(tf.truncated_normal([n_inputs, hidden_size1], stddev=0.05))\n",
    "        b1_h = tf.Variable(tf.constant(0.1, shape=[hidden_size1]))\n",
    "\n",
    "        h1 = tf.nn.tanh(tf.matmul(x, W1_x) + b1_h)\n",
    "        h1_drop = tf.nn.dropout(h1, keep_prob)\n",
    "\n",
    "        W2_x = tf.Variable(tf.truncated_normal([hidden_size1, hidden_size2], stddev=0.05))\n",
    "        b2_h = tf.Variable(tf.constant(0.1, shape=[hidden_size2]))\n",
    "\n",
    "        h2 = tf.nn.tanh(tf.matmul(h1_drop, W2_x) + b2_h)\n",
    "        h2_drop = tf.nn.dropout(h2, keep_prob)\n",
    "\n",
    "        W_h = tf.Variable(tf.zeros([hidden_size2, 1]))\n",
    "\n",
    "        b_y = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "        y = tf.matmul(h2_drop, W_h) + b_y\n",
    "\n",
    "        loss = tf.reduce_mean(tf.square(y - y_))\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.01) # learning rate         \n",
    "        train = optimizer.minimize(loss)\n",
    "\n",
    "        sess = tf.Session()\n",
    "\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        train_batches = len(X_train) // batch_size \n",
    "        t0 = time.time()\n",
    "        prev = None\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(train_batches):\n",
    "                batch_x = X_train[i*batch_size:(i+1)*batch_size]\n",
    "                batch_y = y_train[i*batch_size:(i+1)*batch_size]\n",
    "                sess.run(train, feed_dict={x: batch_x, y_:batch_y, keep_prob:p})\n",
    "            preds = sess.run(y, feed_dict={x: X_valid, keep_prob: 1.0})\n",
    "            curr = np.sqrt(sk.metrics.mean_squared_error(preds, y_valid))\n",
    "            #print prev, curr\n",
    "            if (prev == None):\n",
    "                prev = curr\n",
    "            else:\n",
    "                if curr > prev:\n",
    "                    #print epoch\n",
    "                    break\n",
    "                prev = curr\n",
    "\n",
    "        fit_time = time.time() - t0\n",
    "\n",
    "        preds = sess.run(y, feed_dict={x: X_test, keep_prob: 1.0})\n",
    "        err = np.sqrt(sk.metrics.mean_squared_error(preds, y_test))\n",
    "    return fit_time, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tensorflow_result = {'batch_size':[], 'error':[], 'fit_time':[]}\n",
    "for batch in [128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072]:\n",
    "    tensorflow_result['batch_size'].append(batch)\n",
    "    t, err = fit_predict(batch, X_train, y_train, X_test, y_test, X_valid, y_valid)\n",
    "    tensorflow_result['error'].append(err)\n",
    "    tensorflow_result['fit_time'].append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(tensorflow_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the name and location of the destination file where results will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_file = 'tensorflow.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(output_file, 'w') as fp:\n",
    "    json.dump(tensorflow_result, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "Visit [www.add-for.com](<http://www.add-for.com/IT>) for more tutorials and updates.\n",
    "\n",
    "This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
