{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tensorflow Tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from IPython.display import Image\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"070ad1e5-71c4-4000-b981-d58c286422c9\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = \"1\";\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      Bokeh.$(\"#070ad1e5-71c4-4000-b981-d58c286422c9\").text(\"BokehJS successfully loaded.\");\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"070ad1e5-71c4-4000-b981-d58c286422c9\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '070ad1e5-71c4-4000-b981-d58c286422c9' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.12.3.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.3.min.js'];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      Bokeh.$(\"#070ad1e5-71c4-4000-b981-d58c286422c9\").text(\"BokehJS is loading...\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.3.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.3.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.3.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.3.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === \"1\")) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === \"1\") {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (!force) {\n",
       "      var cell = $(\"#070ad1e5-71c4-4000-b981-d58c286422c9\").parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import bokeh.plotting as bk\n",
    "from bokeh.io import push_notebook\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.models import ColumnDataSource\n",
    "bk.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to install it either with provided binary packages or directly from source. However to compile TensorFlow you need to install Bazel, Google own build tool. We suggest as a starting point ot use the binary package. \n",
    "\n",
    "**<font color='red'>REMARK</font>**: to use GPU capabilities you need to first install [CUDA Toolkit](http://www.nvidia.com/object/cuda_home_new.html) and [cuDNN](https://developer.nvidia.com/cudnn) if you have a cuda enabled device. (Tensorflow now supports Cuda Toolkit 8.0 and cuDNN 5.1) \n",
    "\n",
    "There are few options for binary installation:\n",
    "* [pip](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#pip-installation) \n",
    "* [virtualenv](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#virtualenv-installation)\n",
    "* [anaconda](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#anaconda-installation)\n",
    "* [docker](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#docker-installation)\n",
    "\n",
    "Using a virtual environment, such as with virtualenv and Anaconda, is safer for learning and experimenting. Eventually you will need to use pip, please select the python package **with GPU support**, based on your preferred interpreter (python 2.7, python 3.4 or python 3.5 are all supported) and follow instructions. Through this notebook we will use python 3.5. If you don't have a Cuda GPU it is still possible to use TensorFlow, choose the installation with CPU only support.\n",
    "\n",
    "To use this notebook please install anaconda then follow these instructions:\n",
    "*  `conda update anaconda` \n",
    "* `conda env create -f tensorflow.yml` \n",
    "* `conda activate tensorflow`\n",
    "* `export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc2-cp35-cp35m-linux_x86_64.whl`\n",
    "* `pip install --ignore-installed --upgrade $TF_BINARY_URL`\n",
    "* `mkdir -p ~/.ipython/kernels`\n",
    "* `mv ~/.local/share/jupyter/kernels/python3 ~/.ipython/kernels/tfkernel`\n",
    "* `vim ~/.ipython/kernels/tfkernel/kernel.json` and change `display_name` of the kernel to, for example, TensorFlow\n",
    "* change the kernel of this notebook to **TensorFlow**: `Kernel -> Change kernel -> TensorFlow`\n",
    "\n",
    "**<font color='red'>WARNING</font>**: this library natively supports only Linux and Mac OS. The easiest way to use it under Windows operating system, without compiling it from source, is to use a doker image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Tensorflow Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow represent computations as **graphs**. Nodes in the graph are called **ops** (operations). An op takes zero or more **Tensors** and produces zero or more Tensors as output. A Tensor is a multidimensional array with a specified type. The graph is a description of a computation, in order to actually execute the computation a graph must be launched in a **session**. A session exectue a specific graph on one of the available **devices** (that can be either CPUs or GPUs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Building the Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow programs are usually structured into a construction phase, that assembles a graph, and an execution phase that uses a session to execute ops in the graph.\n",
    "\n",
    "For example, it is possible to represent and train a neural network in the construction phase, and then repeatedly execute a set of training ops in the graph in the execution phase.\n",
    "\n",
    "It is possibile to build a graph by starting with nodes that do not need any input, such as constant nodes. Then it is possible to use the output of the constant node as input to other operations. TensorFlow uses a default graph to which operations are added. It is sufficient for most operations but it is also possible to manage multiple graphs with the `Graph` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = tf.constant([[4.]])\n",
    "b = tf.constant([[3.]])\n",
    "product = tf.matmul(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The code creates three nodes: two constant and an operation (multiplication) that takes two inputs (the two constants) and produces an output (product). To actually procude an output is is necessary to run the graph in a session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Launching a Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Without argument the Session construct uses the default graph. It is necessary to close a session once it is open, otherwise use `with ... as  ...` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 12.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    result = sess.run([product])\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow translates the graph into executable operations and it distributes the computation automatically on available resources. It uses the available **GPU** for as many operations as possible.\n",
    "\n",
    "It is possible to use a **specific device** for a session with `with tf.device(\"/gpu:1\"):` statement. For example previous command execute the graph on the second GPU of the machine. Try to change the string and execute the code on CPU (or another GPU of your machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 12.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        result = sess.run([product])\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an interactive environment like IPython or Jupyter it could be useful to interleave graph construction and run operations, you can use `InteractiveSession`. The only difference with a regular `Session` is that an `InteractiveSession` installs itself as the default session on construction. The methods `Tensor.eval()` and `Operation.run()` will use that session to run ops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Tensor Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow uses **Tensors** to represents all data. Only tensors are passed between ops in the graph. Tensors are an n-dimensional arrays, and, in TensorFlow, are described with rank, shape and type. The rank its the number of dimensions (different from matrix rank), the shape is the number of elements for each dimension and the type is the data type assigned to the tensor.  \n",
    "\n",
    "**Variables** are used to maintain state accross executions of the graph. In this example `state` is initialized to zero and updated each time `update` is run. When using variables, they must be initialized after launching the graph, that is after creating a session. In order to initialize the variables it is necessary to add an init operation that must be run *before* all other operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: 0\n",
      "state: 1\n",
      "state: 2\n",
      "state: 3\n"
     ]
    }
   ],
   "source": [
    "state = tf.Variable(0, name=\"counter\")\n",
    "\n",
    "one = tf.constant(1)\n",
    "new_value = tf.add(state, one)\n",
    "update = tf.assign(state, new_value)\n",
    "\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print('state: ' + str(sess.run(state)))\n",
    "    for _ in range(3):\n",
    "        sess.run(update)\n",
    "        print('state: ' + str(sess.run(state)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `assign` is part of the computational graph as `add` and other operators. They won't produce an effect until `run()` executes the expression. Variables are typically used to represent parameters of a model, for example in neural network they are used to store the weights matrix, that it is updated at every execution of the graph. It is possible to **fetch** more than one variable by passing them simultaneously to the run() command (`session.run([var1, var2])`).\n",
    "\n",
    "So far we have covered how to store values in constant and use variable to update their values. TensorFlow provides also a method to pass a value to the variables with a **feed** mechanism. A feed replace the value of an operation with a value. The typical case is to use **placeholder** to feed the operation with values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 14.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "input1 = tf.placeholder(tf.float32)\n",
    "input2 = tf.placeholder(tf.float32)\n",
    "output = tf.mul(input1, input2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([output], feed_dict={input1:[7.], input2:[2.]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variable declaered as placeholder expects a feed and generate an error if it is not supplied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Classification with a Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Neural Network Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brain neuron has a structure formed by a cell body, an axon were it sends messages to other neuron and dendritic tree were it receives messages from other neuron. A neuron generates outgoing charge (throughout the axon) whenever enough charge has been received (activation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "images/neuron.jpeg",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "image/png": {
       "height": 400,
       "width": 400
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image('images/neuron.jpeg', width=400, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a neural network? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "images/neural_net.jpeg",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image('images/neural_net.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice they learn how to approximate functions. For example, for a classifier, it maps an input to a category `y = f(x)`. They are called networks because they combine many function together. The graph depicts how the function are composed in layers. Each function in the most basic form combines the input with a series of weights. Think about the most basic linear regression `y = wx+b`. `w` is called the **weights matrix** (if we are in higher dimensions) and `b` is the **bias vector**. If we think about the neurons of the brain are activated by a stimuli, it is not suprising that an **activation function** can be applyed to the linear combination of weights, biases and inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "images/Simple Neural Network.png",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "image/png": {
       "height": 500,
       "width": 500
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image('images/Simple Neural Network.png',  width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of all linear combinations and activation functions gives the results, and to quantify how the network is performing we define a **cost function** C.\n",
    "\n",
    "Learning in this contex reduces to find the best weights that allows the neural network to approximate the function, that is to have a small cost function. The algorithm that performs this learning is called **backpropagation**. At the heart of backpropagation is an expression for the partial derivative ∂C/∂w of the cost function C with respect to any weight w (or bias b) in the network. The expression tells us how quickly the cost changes when we change the weights and biases. \n",
    "\n",
    "The **learning algorithm** tells us how much and how often we change the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will learn to classify MNIST handwritten digit images into their correct label (0-9). MNIST is a standard dataset hosted on [Yann LeCun's website](https://www.tensorflow.org/versions/r0.11/tutorials/mnist/beginners/index.html). The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "The importance of classical datasets is twofold. First they are good for people who want to try machine learning techniques while spending minimal efforts on preprocessing and formatting data. Second they are useful for comparing machine learning algorithms, since we know well how they work on these datasets.\n",
    "\n",
    "Each image is 28 pixels by 28 pixels, representing an handwritten number between 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "images/mnist1.png",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image('images/mnist1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret this as a big array of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "images/mnist2.png",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "image/png": {
       "height": 500,
       "width": 500
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image('images/mnist2.png',  width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each image has 28 by 28 pixels, we get a 28x28 array. We can flatten each array into a 28∗28=784 dimensional vector. Each component of the vector is a value between zero and one describing the intensity of the pixel. Thus, we generally think of MNIST as being a collection of 784-dimensional vectors. Flattening the image may throw away information about the structure and it surely does. There are methods that look directly at the 2D image but will be covered in later tutorials.\n",
    "\n",
    "Now, we load the data and we see how it is organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is split into three parts, one for training, one for testingand one for validation. Each dataset is an n-dimensional array with shape [number of examples, 784]. Each example is an image with associated a corresponding label, a number between 0 - 9 that represents the digit depicted in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The MNIST dataset has 10 classes, representing the digits 0 through 9.\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# The MNIST images are always 28x28 pixels.\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train examples:  55000\n",
      "test examples:  10000\n",
      "validation examples:  5000\n"
     ]
    }
   ],
   "source": [
    "print('train examples: ', mnist.train.num_examples)\n",
    "print('test examples: ', mnist.test.num_examples)\n",
    "print('validation examples: ', mnist.validation.num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to classify the digits we will use an output layer with 10 units, one for each digits. For this reason our labels are encoded as \"one-hot vectors\". A one-hot vector is a vector which is 0 in most dimensions, and 1 in a single dimension. In this case the\n",
    "n-th digit will be represented as a vector which is 1 in the n-th dimension. For example, 3 would be [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]. Consequently, mnist.train.labels is a [55000, 10] array of floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(mnist.train.labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 The Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build the network step-by-step, we will discuss its (simple) architecture and how to built it with TensorFlow.\n",
    "\n",
    "The standard way to use TensorFlow is to create the graph first and then run it in a session. A more flexible way to perform a computation is using InteractiveSession. It allows to interleave operations of construction with ones that run the graph, and that is good feature for a notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our simple MLP looks like this: input -> hidden layer -> output classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a `tf.placeholder` with `None` as its first dimension indicates that the first dimension, corresponding to the batch size, can be any size. These placeholder will contain input images and labels and will be fed to the program during the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_placeholder = tf.placeholder(tf.float32, shape=[None, IMAGE_PIXELS])\n",
    "label_placeholder = tf.placeholder(tf.int32, shape=[None, NUM_CLASSES])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to provide a dictionary when feeding the placeholder, we use a small helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, place_1, place_2):\n",
    "    return {place_1: batch[0], place_2: batch[1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden layer has a number of units that can be specified with the parameter `hidden1_units` and it computes a transformation of the linear combination of inputs, weights and biases. There are a lot of transformation available in TensorFlow, one of the most widely used is the Rectified **Linear Unit or ReLU**. Its function is f(x) = max(0, x).\n",
    "\n",
    "The weight matrix is initialized to random values, while the bias vector is initialized to a costant (small) value; it is also good practice to initialize weights of a ReLU with a slightly positive initial bias to avoid \"dead neurons\". A reasonable-sounding idea then might be to set all the initial weights to zero, which we expect to be the \"best guess\" in expectation. This turns out to be a mistake, because if every neuron in the network computes the same output, then they will also all compute the same gradients during backpropagation and undergo the exact same parameter updates. In other words, there is no source of asymmetry between neurons if their weights are initialized to be the same. It is possible and common to initialize the biases to be zero, since the asymmetry breaking is provided by the small random numbers in the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden1_units = 20\n",
    "\n",
    "with tf.name_scope('hidden1'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([IMAGE_PIXELS, hidden1_units], \n",
    "                            stddev=0.1 / math.sqrt(float(IMAGE_PIXELS))),\n",
    "        name='weights')\n",
    "    biases = tf.Variable(tf.zeros([hidden1_units]),\n",
    "                         name='biases')\n",
    "\n",
    "    hidden1 = tf.nn.relu(tf.matmul(image_placeholder, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the use of `tf.name_scope` to define a prefix to the variables. \n",
    "\n",
    "The last stage compute a **softmax** transformation of the hidden layer. The softmax is a generalization of the logistic function, it is used to calculate the probability associated to each class and it is usefull in multiclass classification problems. A softmax regression has two steps: first we add up the evidence of our input being in certain classes, and then we convert that evidence into probabilities. Here a class is one of the 10 possibile digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('softmax_linear'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([hidden1_units, NUM_CLASSES],\n",
    "                            stddev=1.0 / math.sqrt(float(hidden1_units))),\n",
    "        name='weights')\n",
    "    biases = tf.Variable(tf.zeros([NUM_CLASSES]),\n",
    "                         name='biases')\n",
    "    logits = tf.matmul(hidden1, weights) + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we constructed the graph, we need to initialize the variable in the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the cost function, cross entropy in this case. Cross-entropy gives us a way to express how different two probability distributions are. The more different the distributions p and q are, the more the cross-entropy of p with respect to q will be bigger than the entropy of p. Similarly, the more different p is from q, the more the cross-entropy of q with respect to p will be bigger than the entropy of q. If the distributions are the same, this difference will be zero. As the difference grows, it will get bigger.\n",
    "\n",
    "Now it is possible to use TensorFlow automatic differentiation to find the gradients of the cost with respect to each variable. For this example we use basic steepest gradient descent as optimizer.  **Gradient Descent** is a simple procedure, where TensorFlow simply shifts each variable a little bit in the direction that reduces the cost. This *little bit* is the **learning rate** and it is multiplied by the negative of the gradient. For this examples the learning rate is fixed at 0.05. Actually we are using **Stochastic Gradient Descent**. Stochastic means that we are not using all the data to perform a single update of the weights but we are using a subsample at a time. This subsample is called **mini batch** and it is random. Each time we exahaust the training set we can recompute another subsample and so on. Each iteration (that is each time the algorithm consumes all the dataset) is an epoch. At each epoch the function gets better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits,\n",
    "                                                        label_placeholder,\n",
    "                                                        name='xentropy')\n",
    "\n",
    "loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model we first compute the position of the maximum entry in both predicted and real output. This corresponds to the class to which the example belongs. Then we compare the two vector to determine what fraction of the predicted output is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(logits),1), \n",
    "                              tf.argmax(label_placeholder,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code displays the training loss, the validation loss and test loss function. To actually see the graph, first start the algorithm. The plots will updates dinamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_accuracy_vector = []\n",
    "valid_accuracy_vector = []\n",
    "test_accuracy_vector = []\n",
    "cost_vector = []\n",
    "input_vector = []\n",
    "\n",
    "s_train = ColumnDataSource(data=dict(x=input_vector, y=train_accuracy_vector))\n",
    "s_valid = ColumnDataSource(data=dict(x=input_vector, y=valid_accuracy_vector))\n",
    "s_test = ColumnDataSource(data=dict(x=input_vector, y=test_accuracy_vector))\n",
    "s_cost = ColumnDataSource(data=dict(x=input_vector, y=cost_vector))\n",
    "\n",
    "fig1 = bk.figure(plot_width=500, \n",
    "                plot_height=300,\n",
    "                x_axis_label='Epochs',\n",
    "                y_axis_label='Precision (%)')\n",
    "\n",
    "fig1.line('x', 'y', source=s_train, legend='training')\n",
    "fig1.line('x', 'y', source=s_valid, color='red', legend='validation')\n",
    "fig1.line('x', 'y', source=s_test, color='green', legend='test')\n",
    "\n",
    "fig2 = bk.figure(plot_width=500, \n",
    "                plot_height=300,\n",
    "                x_axis_label='Epochs',\n",
    "                y_axis_label='Cost')\n",
    "\n",
    "fig2.line('x', 'y', source=s_cost)\n",
    "\n",
    "p = gridplot([[fig1], [fig2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_c(x_vec, y_vec):\n",
    "    s_cost.data['x'] = x_vec\n",
    "    s_cost.data['y'] = y_vec\n",
    "    push_notebook()\n",
    "    \n",
    "def update(x_vec, t_vec, v_vec, e_vec):\n",
    "    s_train.data['x'] = x_vec\n",
    "    s_valid.data['x'] = x_vec\n",
    "    s_test.data['x'] = x_vec\n",
    "    s_train.data['y'] = t_vec\n",
    "    s_valid.data['y'] = v_vec\n",
    "    s_test.data['y'] = e_vec\n",
    "    push_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <div class=\"plotdiv\" id=\"3c0fbf85-d51f-4e1c-b34b-9ab1d6f7a817\"></div>\n",
       "    </div>\n",
       "<script type=\"text/javascript\">\n",
       "  \n",
       "  (function(global) {\n",
       "    function now() {\n",
       "      return new Date();\n",
       "    }\n",
       "  \n",
       "    var force = \"\";\n",
       "  \n",
       "    if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force !== \"\") {\n",
       "      window._bokeh_onload_callbacks = [];\n",
       "      window._bokeh_is_loading = undefined;\n",
       "    }\n",
       "  \n",
       "  \n",
       "    \n",
       "    if (typeof (window._bokeh_timeout) === \"undefined\" || force !== \"\") {\n",
       "      window._bokeh_timeout = Date.now() + 0;\n",
       "      window._bokeh_failed_load = false;\n",
       "    }\n",
       "  \n",
       "    var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "       \"<div style='background-color: #fdd'>\\n\"+\n",
       "       \"<p>\\n\"+\n",
       "       \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "       \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "       \"</p>\\n\"+\n",
       "       \"<ul>\\n\"+\n",
       "       \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "       \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "       \"</ul>\\n\"+\n",
       "       \"<code>\\n\"+\n",
       "       \"from bokeh.resources import INLINE\\n\"+\n",
       "       \"output_notebook(resources=INLINE)\\n\"+\n",
       "       \"</code>\\n\"+\n",
       "       \"</div>\"}};\n",
       "  \n",
       "    function display_loaded() {\n",
       "      if (window.Bokeh !== undefined) {\n",
       "        Bokeh.$(\"#3c0fbf85-d51f-4e1c-b34b-9ab1d6f7a817\").text(\"BokehJS successfully loaded.\");\n",
       "      } else if (Date.now() < window._bokeh_timeout) {\n",
       "        setTimeout(display_loaded, 100)\n",
       "      }\n",
       "    }if ((window.Jupyter !== undefined) && Jupyter.notebook.kernel) {\n",
       "      comm_manager = Jupyter.notebook.kernel.comm_manager\n",
       "      comm_manager.register_target(\"1886a552-212d-4759-ad9c-a29caa313ed0\", function () {});\n",
       "    }\n",
       "  \n",
       "    function run_callbacks() {\n",
       "      window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "      delete window._bokeh_onload_callbacks\n",
       "      console.info(\"Bokeh: all callbacks have finished\");\n",
       "    }\n",
       "  \n",
       "    function load_libs(js_urls, callback) {\n",
       "      window._bokeh_onload_callbacks.push(callback);\n",
       "      if (window._bokeh_is_loading > 0) {\n",
       "        console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "        return null;\n",
       "      }\n",
       "      if (js_urls == null || js_urls.length === 0) {\n",
       "        run_callbacks();\n",
       "        return null;\n",
       "      }\n",
       "      console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "      window._bokeh_is_loading = js_urls.length;\n",
       "      for (var i = 0; i < js_urls.length; i++) {\n",
       "        var url = js_urls[i];\n",
       "        var s = document.createElement('script');\n",
       "        s.src = url;\n",
       "        s.async = false;\n",
       "        s.onreadystatechange = s.onload = function() {\n",
       "          window._bokeh_is_loading--;\n",
       "          if (window._bokeh_is_loading === 0) {\n",
       "            console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "            run_callbacks()\n",
       "          }\n",
       "        };\n",
       "        s.onerror = function() {\n",
       "          console.warn(\"failed to load library \" + url);\n",
       "        };\n",
       "        console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      }\n",
       "    };var element = document.getElementById(\"3c0fbf85-d51f-4e1c-b34b-9ab1d6f7a817\");\n",
       "    if (element == null) {\n",
       "      console.log(\"Bokeh: ERROR: autoload.js configured with elementid '3c0fbf85-d51f-4e1c-b34b-9ab1d6f7a817' but no matching script tag was found. \")\n",
       "      return false;\n",
       "    }\n",
       "  \n",
       "    var js_urls = [];\n",
       "  \n",
       "    var inline_js = [\n",
       "      function(Bokeh) {\n",
       "        Bokeh.$(function() {\n",
       "            var docs_json = {\"acaf78f3-b32e-4332-90f0-a1d43916ae53\":{\"roots\":{\"references\":[{\"attributes\":{\"data_source\":{\"id\":\"a2a971aa-3d88-4ede-b8f2-37e36744b64c\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"ffb8ceac-0f3b-437f-8242-e8e84b69636c\",\"type\":\"Line\"},\"hover_glyph\":null,\"nonselection_glyph\":{\"id\":\"28a81857-169b-4ed9-9291-5646184aed05\",\"type\":\"Line\"},\"selection_glyph\":null},\"id\":\"e9bb2e83-6517-4c4c-bbdf-d29da2cfa103\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"49927107-3834-4360-a914-350b3a28fe0f\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1797fbdb-7a15-41c2-95cb-7e217f28d523\",\"type\":\"BasicTicker\"}},\"id\":\"f5e1d239-e9fa-49ac-93bb-05f454fe54f7\",\"type\":\"Grid\"},{\"attributes\":{\"plot\":{\"id\":\"49927107-3834-4360-a914-350b3a28fe0f\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"724bf9b1-4f74-42e4-8b6b-9610446e2bb4\",\"type\":\"HelpTool\"},{\"attributes\":{},\"id\":\"4ad808d4-a869-4551-8dd0-3892051838d2\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"overlay\":{\"id\":\"b5242055-7d00-4c6b-ac1c-fce02e570d6a\",\"type\":\"BoxAnnotation\"},\"plot\":{\"id\":\"49927107-3834-4360-a914-350b3a28fe0f\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"2b612f0f-abb5-4616-83ab-746dff289e8d\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"80881fed-bca8-4787-939d-9a8536d7faa6\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"y\",\"x\"],\"data\":{\"x\":[],\"y\":[]}},\"id\":\"d250e08f-90c3-4b05-b75e-435910a84440\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"plot\":{\"id\":\"3d4e72dc-ef28-4d97-b164-7272645bc959\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"bed1c6c4-7cbb-4b8b-8ede-e24cc20a7851\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"plot\":{\"id\":\"3d4e72dc-ef28-4d97-b164-7272645bc959\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"9df4e91d-fe10-41a4-98fa-45e85e35a30b\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"3e169606-c0f6-4167-ad37-cfec73cbe0fe\",\"type\":\"ToolEvents\"},{\"attributes\":{\"plot\":{\"id\":\"49927107-3834-4360-a914-350b3a28fe0f\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"e8eb6c01-1af7-4ea0-9b73-93676618f66b\",\"type\":\"ResetTool\"},{\"attributes\":{\"axis_label\":\"Precision (%)\",\"formatter\":{\"id\":\"185e9a46-99be-4236-a2b4-8174342db963\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"49927107-3834-4360-a914-350b3a28fe0f\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1797fbdb-7a15-41c2-95cb-7e217f28d523\",\"type\":\"BasicTicker\"}},\"id\":\"eadf5ac7-1d39-4f55-87d3-8d358878ae9e\",\"type\":\"LinearAxis\"},{\"attributes\":{\"data_source\":{\"id\":\"ad46006d-298a-4cb7-ab70-67dea3c5a58f\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"0c623fc0-6d71-4827-a238-6bdf934ca00c\",\"type\":\"Line\"},\"hover_glyph\":null,\"nonselection_glyph\":{\"id\":\"2d2ec0dc-3f7a-4858-af8a-362ee64425bb\",\"type\":\"Line\"},\"selection_glyph\":null},\"id\":\"229da578-e338-4af1-9ea1-240375a6eff1\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"2d2ec0dc-3f7a-4858-af8a-362ee64425bb\",\"type\":\"Line\"},{\"attributes\":{\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"91b30caa-0509-414c-8c1f-a4a982221664\",\"type\":\"Line\"},{\"attributes\":{\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"3d1f4649-4740-4039-b018-a75ada01870b\",\"type\":\"Line\"},{\"attributes\":{\"plot\":{\"id\":\"3d4e72dc-ef28-4d97-b164-7272645bc959\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"b02ec5bc-0f40-46c9-a43d-aa37634ebba1\",\"type\":\"BasicTicker\"}},\"id\":\"5efe48bd-1e77-46e3-a6b4-5567647acf6b\",\"type\":\"Grid\"},{\"attributes\":{\"label\":{\"value\":\"validation\"},\"renderers\":[{\"id\":\"839934b7-96be-4914-9449-5b4ecb7e3ac4\",\"type\":\"GlyphRenderer\"}]},\"id\":\"58811c19-0e40-4748-9ba0-300771799402\",\"type\":\"LegendItem\"},{\"attributes\":{\"below\":[{\"id\":\"797b625f-addf-45aa-843d-2f40ca9bae06\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"1a28282c-68be-43fd-a079-fbf965e82ba5\",\"type\":\"LinearAxis\"}],\"plot_height\":300,\"plot_width\":500,\"renderers\":[{\"id\":\"797b625f-addf-45aa-843d-2f40ca9bae06\",\"type\":\"LinearAxis\"},{\"id\":\"5efe48bd-1e77-46e3-a6b4-5567647acf6b\",\"type\":\"Grid\"},{\"id\":\"1a28282c-68be-43fd-a079-fbf965e82ba5\",\"type\":\"LinearAxis\"},{\"id\":\"2c370e74-532a-4d94-8c88-d7a123a07b8a\",\"type\":\"Grid\"},{\"id\":\"1c6b2585-2dfc-4558-a98b-c47398bc6c7b\",\"type\":\"BoxAnnotation\"},{\"id\":\"e9bb2e83-6517-4c4c-bbdf-d29da2cfa103\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"c9b0b2c5-7521-46cb-9d14-3ea1dab9f646\",\"type\":\"Title\"},\"tool_events\":{\"id\":\"3e169606-c0f6-4167-ad37-cfec73cbe0fe\",\"type\":\"ToolEvents\"},\"toolbar\":{\"id\":\"8adf0af7-4bf9-4970-9768-33a3ee843d53\",\"type\":\"Toolbar\"},\"toolbar_location\":null,\"x_range\":{\"id\":\"79d727c4-e7b6-4f12-ae0c-1fa96193e1b9\",\"type\":\"DataRange1d\"},\"y_range\":{\"id\":\"92f20c96-7752-4f14-bf2b-791e5b3ee42f\",\"type\":\"DataRange1d\"}},\"id\":\"3d4e72dc-ef28-4d97-b164-7272645bc959\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"2758ddf7-2268-4bae-8c24-5551f4b742b8\",\"type\":\"BasicTicker\"},{\"attributes\":{\"items\":[{\"id\":\"3bba27cb-43c0-4386-9619-7c3982ebc16e\",\"type\":\"LegendItem\"},{\"id\":\"58811c19-0e40-4748-9ba0-300771799402\",\"type\":\"LegendItem\"},{\"id\":\"cc942026-537a-4c2b-9a55-09063f73d25d\",\"type\":\"LegendItem\"}],\"plot\":{\"id\":\"49927107-3834-4360-a914-350b3a28fe0f\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"b29e785d-aac8-404d-a4e1-1f2778ed3545\",\"type\":\"Legend\"},{\"attributes\":{\"label\":{\"value\":\"training\"},\"renderers\":[{\"id\":\"882f5ad3-2ad1-4bda-8737-6705e8cbf8c2\",\"type\":\"GlyphRenderer\"}]},\"id\":\"3bba27cb-43c0-4386-9619-7c3982ebc16e\",\"type\":\"LegendItem\"},{\"attributes\":{\"line_color\":{\"value\":\"red\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"7e039079-37cc-462c-b51b-a4d1620b32e7\",\"type\":\"Line\"},{\"attributes\":{\"line_color\":{\"value\":\"green\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"0c623fc0-6d71-4827-a238-6bdf934ca00c\",\"type\":\"Line\"},{\"attributes\":{\"overlay\":{\"id\":\"1c6b2585-2dfc-4558-a98b-c47398bc6c7b\",\"type\":\"BoxAnnotation\"},\"plot\":{\"id\":\"3d4e72dc-ef28-4d97-b164-7272645bc959\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"dd1f809d-63bb-4609-98fb-8ec4e5396fe7\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"children\":[{\"id\":\"49927107-3834-4360-a914-350b3a28fe0f\",\"subtype\":\"Figure\",\"type\":\"Plot\"}]},\"id\":\"c016eae8-074b-4ea7-960d-5fa22be4f5d8\",\"type\":\"Row\"},{\"attributes\":{},\"id\":\"10f1b4d7-2745-43e3-b497-dc24f2d63fe8\",\"type\":\"ToolEvents\"},{\"attributes\":{\"plot\":null,\"text\":null},\"id\":\"c9b0b2c5-7521-46cb-9d14-3ea1dab9f646\",\"type\":\"Title\"},{\"attributes\":{\"children\":[{\"id\":\"be05ca06-0712-43f3-bfe0-250bbca442e8\",\"type\":\"ToolbarBox\"},{\"id\":\"6f705f4c-ac2b-421a-9f65-f9faee09d393\",\"type\":\"Column\"}]},\"id\":\"8ee794f6-85eb-4cba-a4c2-fb3a72b782b2\",\"type\":\"Column\"},{\"attributes\":{\"below\":[{\"id\":\"69d8a510-f823-4852-9bf1-840b291cadf0\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"eadf5ac7-1d39-4f55-87d3-8d358878ae9e\",\"type\":\"LinearAxis\"}],\"plot_height\":300,\"plot_width\":500,\"renderers\":[{\"id\":\"69d8a510-f823-4852-9bf1-840b291cadf0\",\"type\":\"LinearAxis\"},{\"id\":\"3738fb31-9181-4de8-9521-87e0dd475a2f\",\"type\":\"Grid\"},{\"id\":\"eadf5ac7-1d39-4f55-87d3-8d358878ae9e\",\"type\":\"LinearAxis\"},{\"id\":\"f5e1d239-e9fa-49ac-93bb-05f454fe54f7\",\"type\":\"Grid\"},{\"id\":\"b5242055-7d00-4c6b-ac1c-fce02e570d6a\",\"type\":\"BoxAnnotation\"},{\"id\":\"b29e785d-aac8-404d-a4e1-1f2778ed3545\",\"type\":\"Legend\"},{\"id\":\"882f5ad3-2ad1-4bda-8737-6705e8cbf8c2\",\"type\":\"GlyphRenderer\"},{\"id\":\"839934b7-96be-4914-9449-5b4ecb7e3ac4\",\"type\":\"GlyphRenderer\"},{\"id\":\"229da578-e338-4af1-9ea1-240375a6eff1\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"ca6f777a-88f4-490b-a875-5448113df07c\",\"type\":\"Title\"},\"tool_events\":{\"id\":\"10f1b4d7-2745-43e3-b497-dc24f2d63fe8\",\"type\":\"ToolEvents\"},\"toolbar\":{\"id\":\"0d867a6a-93d9-4e14-b6c1-3f7adaf82820\",\"type\":\"Toolbar\"},\"toolbar_location\":null,\"x_range\":{\"id\":\"758dccc8-4109-4192-8341-52e0af14176d\",\"type\":\"DataRange1d\"},\"y_range\":{\"id\":\"923b9123-16dd-4ae8-a9fa-66aafa7851df\",\"type\":\"DataRange1d\"}},\"id\":\"49927107-3834-4360-a914-350b3a28fe0f\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"data_source\":{\"id\":\"d250e08f-90c3-4b05-b75e-435910a84440\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"3d1f4649-4740-4039-b018-a75ada01870b\",\"type\":\"Line\"},\"hover_glyph\":null,\"nonselection_glyph\":{\"id\":\"e9c0c82e-c6ad-44e4-a549-148fd33f4fb3\",\"type\":\"Line\"},\"selection_glyph\":null},\"id\":\"882f5ad3-2ad1-4bda-8737-6705e8cbf8c2\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null},\"id\":\"923b9123-16dd-4ae8-a9fa-66aafa7851df\",\"type\":\"DataRange1d\"},{\"attributes\":{\"plot\":{\"id\":\"49927107-3834-4360-a914-350b3a28fe0f\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"da2b89e3-5f43-4c3b-bd0b-e3165da3c1d7\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"y\",\"x\"],\"data\":{\"x\":[],\"y\":[]}},\"id\":\"a2a971aa-3d88-4ede-b8f2-37e36744b64c\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"plot\":{\"id\":\"3d4e72dc-ef28-4d97-b164-7272645bc959\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"0e2b4319-5c01-40f1-a94f-866d0d026298\",\"type\":\"HelpTool\"},{\"attributes\":{},\"id\":\"1797fbdb-7a15-41c2-95cb-7e217f28d523\",\"type\":\"BasicTicker\"},{\"attributes\":{\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"28a81857-169b-4ed9-9291-5646184aed05\",\"type\":\"Line\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"y\",\"x\"],\"data\":{\"x\":[],\"y\":[]}},\"id\":\"ad46006d-298a-4cb7-ab70-67dea3c5a58f\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"axis_label\":\"Cost\",\"formatter\":{\"id\":\"4ad808d4-a869-4551-8dd0-3892051838d2\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"3d4e72dc-ef28-4d97-b164-7272645bc959\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"a991746a-a579-4ebd-9a02-fb9a0650531b\",\"type\":\"BasicTicker\"}},\"id\":\"1a28282c-68be-43fd-a079-fbf965e82ba5\",\"type\":\"LinearAxis\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1c6b2585-2dfc-4558-a98b-c47398bc6c7b\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"y\",\"x\"],\"data\":{\"x\":[],\"y\":[]}},\"id\":\"344a087b-9997-4d19-906e-2d49601142a8\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"plot\":{\"id\":\"49927107-3834-4360-a914-350b3a28fe0f\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"2758ddf7-2268-4bae-8c24-5551f4b742b8\",\"type\":\"BasicTicker\"}},\"id\":\"3738fb31-9181-4de8-9521-87e0dd475a2f\",\"type\":\"Grid\"},{\"attributes\":{\"children\":[{\"id\":\"3d4e72dc-ef28-4d97-b164-7272645bc959\",\"subtype\":\"Figure\",\"type\":\"Plot\"}]},\"id\":\"58d155b1-109f-4f86-b958-a4d9fd13f1aa\",\"type\":\"Row\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"d16935f4-a248-4054-a127-4ba57b923c6d\",\"type\":\"PanTool\"},{\"id\":\"da2b89e3-5f43-4c3b-bd0b-e3165da3c1d7\",\"type\":\"WheelZoomTool\"},{\"id\":\"2b612f0f-abb5-4616-83ab-746dff289e8d\",\"type\":\"BoxZoomTool\"},{\"id\":\"b54278ab-5bc2-49c2-99cb-5f3f1a53023f\",\"type\":\"SaveTool\"},{\"id\":\"e8eb6c01-1af7-4ea0-9b73-93676618f66b\",\"type\":\"ResetTool\"},{\"id\":\"724bf9b1-4f74-42e4-8b6b-9610446e2bb4\",\"type\":\"HelpTool\"}]},\"id\":\"0d867a6a-93d9-4e14-b6c1-3f7adaf82820\",\"type\":\"Toolbar\"},{\"attributes\":{\"sizing_mode\":\"scale_width\",\"toolbar_location\":\"above\",\"tools\":[{\"id\":\"d16935f4-a248-4054-a127-4ba57b923c6d\",\"type\":\"PanTool\"},{\"id\":\"da2b89e3-5f43-4c3b-bd0b-e3165da3c1d7\",\"type\":\"WheelZoomTool\"},{\"id\":\"2b612f0f-abb5-4616-83ab-746dff289e8d\",\"type\":\"BoxZoomTool\"},{\"id\":\"b54278ab-5bc2-49c2-99cb-5f3f1a53023f\",\"type\":\"SaveTool\"},{\"id\":\"e8eb6c01-1af7-4ea0-9b73-93676618f66b\",\"type\":\"ResetTool\"},{\"id\":\"724bf9b1-4f74-42e4-8b6b-9610446e2bb4\",\"type\":\"HelpTool\"},{\"id\":\"cdebe82b-637f-431b-87b0-b3e1cb5fb36c\",\"type\":\"PanTool\"},{\"id\":\"bed1c6c4-7cbb-4b8b-8ede-e24cc20a7851\",\"type\":\"WheelZoomTool\"},{\"id\":\"dd1f809d-63bb-4609-98fb-8ec4e5396fe7\",\"type\":\"BoxZoomTool\"},{\"id\":\"9df4e91d-fe10-41a4-98fa-45e85e35a30b\",\"type\":\"SaveTool\"},{\"id\":\"b95ffb95-8f48-4817-b04d-c31a710c2c53\",\"type\":\"ResetTool\"},{\"id\":\"0e2b4319-5c01-40f1-a94f-866d0d026298\",\"type\":\"HelpTool\"}]},\"id\":\"be05ca06-0712-43f3-bfe0-250bbca442e8\",\"type\":\"ToolbarBox\"},{\"attributes\":{},\"id\":\"b02ec5bc-0f40-46c9-a43d-aa37634ebba1\",\"type\":\"BasicTicker\"},{\"attributes\":{\"callback\":null},\"id\":\"79d727c4-e7b6-4f12-ae0c-1fa96193e1b9\",\"type\":\"DataRange1d\"},{\"attributes\":{\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"ffb8ceac-0f3b-437f-8242-e8e84b69636c\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"8d520289-8cf3-4fdb-9728-0319c7f0a4fb\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"label\":{\"value\":\"test\"},\"renderers\":[{\"id\":\"229da578-e338-4af1-9ea1-240375a6eff1\",\"type\":\"GlyphRenderer\"}]},\"id\":\"cc942026-537a-4c2b-9a55-09063f73d25d\",\"type\":\"LegendItem\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"3d4e72dc-ef28-4d97-b164-7272645bc959\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"a991746a-a579-4ebd-9a02-fb9a0650531b\",\"type\":\"BasicTicker\"}},\"id\":\"2c370e74-532a-4d94-8c88-d7a123a07b8a\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"a991746a-a579-4ebd-9a02-fb9a0650531b\",\"type\":\"BasicTicker\"},{\"attributes\":{\"callback\":null},\"id\":\"758dccc8-4109-4192-8341-52e0af14176d\",\"type\":\"DataRange1d\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"b5242055-7d00-4c6b-ac1c-fce02e570d6a\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"plot\":null,\"text\":null},\"id\":\"ca6f777a-88f4-490b-a875-5448113df07c\",\"type\":\"Title\"},{\"attributes\":{\"axis_label\":\"Epochs\",\"formatter\":{\"id\":\"80881fed-bca8-4787-939d-9a8536d7faa6\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"49927107-3834-4360-a914-350b3a28fe0f\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"2758ddf7-2268-4bae-8c24-5551f4b742b8\",\"type\":\"BasicTicker\"}},\"id\":\"69d8a510-f823-4852-9bf1-840b291cadf0\",\"type\":\"LinearAxis\"},{\"attributes\":{\"plot\":{\"id\":\"3d4e72dc-ef28-4d97-b164-7272645bc959\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"b95ffb95-8f48-4817-b04d-c31a710c2c53\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"185e9a46-99be-4236-a2b4-8174342db963\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"e9c0c82e-c6ad-44e4-a549-148fd33f4fb3\",\"type\":\"Line\"},{\"attributes\":{\"data_source\":{\"id\":\"344a087b-9997-4d19-906e-2d49601142a8\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"7e039079-37cc-462c-b51b-a4d1620b32e7\",\"type\":\"Line\"},\"hover_glyph\":null,\"nonselection_glyph\":{\"id\":\"91b30caa-0509-414c-8c1f-a4a982221664\",\"type\":\"Line\"},\"selection_glyph\":null},\"id\":\"839934b7-96be-4914-9449-5b4ecb7e3ac4\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"plot\":{\"id\":\"3d4e72dc-ef28-4d97-b164-7272645bc959\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"cdebe82b-637f-431b-87b0-b3e1cb5fb36c\",\"type\":\"PanTool\"},{\"attributes\":{\"plot\":{\"id\":\"49927107-3834-4360-a914-350b3a28fe0f\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"b54278ab-5bc2-49c2-99cb-5f3f1a53023f\",\"type\":\"SaveTool\"},{\"attributes\":{\"callback\":null},\"id\":\"92f20c96-7752-4f14-bf2b-791e5b3ee42f\",\"type\":\"DataRange1d\"},{\"attributes\":{\"plot\":{\"id\":\"49927107-3834-4360-a914-350b3a28fe0f\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"d16935f4-a248-4054-a127-4ba57b923c6d\",\"type\":\"PanTool\"},{\"attributes\":{\"axis_label\":\"Epochs\",\"formatter\":{\"id\":\"8d520289-8cf3-4fdb-9728-0319c7f0a4fb\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"3d4e72dc-ef28-4d97-b164-7272645bc959\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"b02ec5bc-0f40-46c9-a43d-aa37634ebba1\",\"type\":\"BasicTicker\"}},\"id\":\"797b625f-addf-45aa-843d-2f40ca9bae06\",\"type\":\"LinearAxis\"},{\"attributes\":{\"children\":[{\"id\":\"c016eae8-074b-4ea7-960d-5fa22be4f5d8\",\"type\":\"Row\"},{\"id\":\"58d155b1-109f-4f86-b958-a4d9fd13f1aa\",\"type\":\"Row\"}]},\"id\":\"6f705f4c-ac2b-421a-9f65-f9faee09d393\",\"type\":\"Column\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"cdebe82b-637f-431b-87b0-b3e1cb5fb36c\",\"type\":\"PanTool\"},{\"id\":\"bed1c6c4-7cbb-4b8b-8ede-e24cc20a7851\",\"type\":\"WheelZoomTool\"},{\"id\":\"dd1f809d-63bb-4609-98fb-8ec4e5396fe7\",\"type\":\"BoxZoomTool\"},{\"id\":\"9df4e91d-fe10-41a4-98fa-45e85e35a30b\",\"type\":\"SaveTool\"},{\"id\":\"b95ffb95-8f48-4817-b04d-c31a710c2c53\",\"type\":\"ResetTool\"},{\"id\":\"0e2b4319-5c01-40f1-a94f-866d0d026298\",\"type\":\"HelpTool\"}]},\"id\":\"8adf0af7-4bf9-4970-9768-33a3ee843d53\",\"type\":\"Toolbar\"}],\"root_ids\":[\"8ee794f6-85eb-4cba-a4c2-fb3a72b782b2\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.3\"}};\n",
       "            var render_items = [{\"docid\":\"acaf78f3-b32e-4332-90f0-a1d43916ae53\",\"elementid\":\"3c0fbf85-d51f-4e1c-b34b-9ab1d6f7a817\",\"modelid\":\"8ee794f6-85eb-4cba-a4c2-fb3a72b782b2\",\"notebook_comms_target\":\"1886a552-212d-4759-ad9c-a29caa313ed0\"}];\n",
       "            \n",
       "            Bokeh.embed.embed_items(docs_json, render_items);\n",
       "        });\n",
       "      },\n",
       "      function(Bokeh) {\n",
       "      }\n",
       "    ];\n",
       "  \n",
       "    function run_inline_js() {\n",
       "      \n",
       "      if ((window.Bokeh !== undefined) || (force === \"1\")) {\n",
       "        for (var i = 0; i < inline_js.length; i++) {\n",
       "          inline_js[i](window.Bokeh);\n",
       "        }if (force === \"1\") {\n",
       "          display_loaded();\n",
       "        }} else if (Date.now() < window._bokeh_timeout) {\n",
       "        setTimeout(run_inline_js, 100);\n",
       "      } else if (!window._bokeh_failed_load) {\n",
       "        console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "        window._bokeh_failed_load = true;\n",
       "      } else if (!force) {\n",
       "        var cell = $(\"#3c0fbf85-d51f-4e1c-b34b-9ab1d6f7a817\").parents('.cell').data().cell;\n",
       "        cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "      }\n",
       "  \n",
       "    }\n",
       "  \n",
       "    if (window._bokeh_is_loading === 0) {\n",
       "      console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "      run_inline_js();\n",
       "    } else {\n",
       "      load_libs(js_urls, function() {\n",
       "        console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "        run_inline_js();\n",
       "      });\n",
       "    }\n",
       "  }(this));\n",
       "</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ha1 = bk.show(p, notebook_handle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we are ready to launch the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "batch_size = 64\n",
    "train_batches = mnist.train.num_examples // batch_size\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "    train_costs = []\n",
    "    train_accuracy = []\n",
    "    for i in range(train_batches):\n",
    "        feed_dict = get_feed_dict(mnist.train.next_batch(batch_size), \n",
    "                                  image_placeholder, \n",
    "                                  label_placeholder)\n",
    "        _, loss_value = sess.run([train_op, loss], \n",
    "                                 feed_dict= feed_dict)\n",
    "        train_accuracy.append(accuracy.eval(feed_dict=feed_dict))\n",
    "        train_costs.append(loss_value)\n",
    "\n",
    "    train_accuracy_vector.append(np.mean(train_accuracy) * 100)\n",
    "    cost_vector.append(np.mean(train_costs))\n",
    "    update_c(range(epoch), cost_vector)\n",
    "     \n",
    "    valid_accuracy_vector.append(accuracy.eval(feed_dict={image_placeholder: mnist.validation.images, \n",
    "                                                          label_placeholder: mnist.validation.labels}) * 100)\n",
    "    test_accuracy_vector.append(accuracy.eval(feed_dict={image_placeholder: mnist.test.images, \n",
    "                                                         label_placeholder: mnist.test.labels}) * 100)\n",
    "    \n",
    "    update(range(epoch), train_accuracy_vector, valid_accuracy_vector, test_accuracy_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**REMARK** Epochs are calculated directly by the mnist object. Whenver the number of batches requested is greather than the number of examples it automatically updates the number of epochs. The object shuffles also the training set for better learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.956\n"
     ]
    }
   ],
   "source": [
    "print(accuracy.eval(feed_dict={image_placeholder: mnist.test.images, \n",
    "                               label_placeholder: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 TensorFlow contrib.learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module **learn** in the contrib section of the repository of TensorFlow was formerly known as skflow and it is meant to provide ease of use and modularity, for beginners and for integrating TensorFlow models in existing code. It has an interface similar to scikit-learn and it is possible to create a Neural Network with few line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import learn\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = learn.DNNClassifier(hidden_units=[hidden1_units],\n",
    "                                 n_classes=NUM_CLASSES,\n",
    "                                 feature_columns=learn.infer_real_valued_columns_from_input(mnist.train.images.astype(np.float32)),\n",
    "                                 optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.05)) \n",
    "                                 # model_dir=\"logdir/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just one line of code we are able to create the same neural network that took us many line of code to compute, and with just one line we can fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Estimator(params={'activation_fn': <function relu at 0x7fe26cd16c80>, 'feature_columns': [_RealValuedColumn(column_name='', dimension=784, default_value=None, dtype=tf.float32, normalizer=None)], 'num_ps_replicas': 0, 'dropout': None, 'gradient_clip_norm': None, 'hidden_units': [20], 'optimizer': <tensorflow.python.training.gradient_descent.GradientDescentOptimizer object at 0x7fe2cc821b70>, 'enable_centered_bias': True, 'weight_column_name': None, 'n_classes': 10})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(mnist.train.images, \n",
    "               np.argmax(mnist.train.labels, axis=1).astype(np.int64), \n",
    "               batch_size=batch_size, \n",
    "               steps=3000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is similar to the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.933700\n"
     ]
    }
   ],
   "source": [
    "accuracy_score = classifier.evaluate(x=mnist.test.images.astype(np.float32), \n",
    "                                     y=np.argmax(mnist.test.labels, axis=1).astype(np.int64))[\"accuracy\"]\n",
    "print('Accuracy: {0:f}'.format(accuracy_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn has many models, for example it is possible to create a linear classification model or even a Random Forest! It is also possible to create an highly customized model by providing a model function to an `Estimator` object. However if the model is highly complex maybe it is best to use plain TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Deeper Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the simple MLP is pretty good, but what if we want to improve it? Increasing the number of layers does not improve the accuracy, but why? This is known as the vanishing or exploding gradient problems. That is, the learning procedure based on gradients is unstable if we have many layers, resulting in the last layers to learn slowly. For this reason in order to train deeper network we must use another kind of network, the **Convolutional Neural Network**. In practice to avoid the vanishing gradient problem we use a specialized network with fewer parameters, that takes advantage of spatial locality of the images.\n",
    "\n",
    "The following code defines the usual placeholder for the input examples and associated labels, as well as two helper functions to construct weight variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. Let's look at each of these ideas in turn. \n",
    "\n",
    "Local receptive fields: In the fully-connected layers shown earlier, the inputs were depicted as a vertical line of neurons. In a convolutional net, it'll help to think instead of the inputs as a 28×28 square of neurons, whose values correspond to the 28×28 pixel intensities we're using as inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "images/receptive.png",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image('images/receptive.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will connect the input pixels to a layer of hidden neurons. But we don't connect every input pixel to every hidden neuron. Instead, we only make connections in small, localized regions of the input image.\n",
    "\n",
    "To be more precise, each neuron in the first hidden layer will be connected to a small region of the input neurons, say, for example, a 5×5 region, corresponding to 25 input pixels. That region in the input image is called the **local receptive field** for the hidden neuron. I've shown the local receptive field being moved by one pixel at a time. In fact, sometimes a different stride length is used. For instance, we might move the local receptive field 2 pixels to the right (or down), in which case we'd say a stride length of 2 is used. \n",
    "\n",
    "Each hidden neuron has a bias and 5×5 weights connected to its local receptive field. What I did not yet mention is that we're going to use the same weights and bias for each of the 24×24 hidden neurons. This means that all the neurons in the first hidden layer detect exactly the same feature just at different locations in the input image. They **Share weights and biases**. For this reason, we sometimes call the map from the input layer to the hidden layer a **feature map**. We call the weights defining the feature map the shared weights. And we call the bias defining the feature map in this way the shared bias. The shared weights and bias are often said to define a kernel or filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "images/feature_map.png",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "image/png": {
       "height": 700,
       "width": 700
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    " Image('images/feature_map.png', width=700, height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pooling layers**: In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is simplify the information in the output from the convolutional layer. Pool layers perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as [14x14x32]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "images/maxpool.jpeg",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "image/png": {
       "height": 600,
       "width": 600
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image('images/maxpool.jpeg', width=600, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape of the input so it resemble a 2D image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1,28,28,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement our first layer. It will consist of convolution, followed by max pooling. The convolutional will compute 32 features for each 5x5 patch. Its weight tensor will have a shape of [5, 5, 1, 32]. The first two dimensions are the patch size, the next is the number of input channels, and the last is the number of output channels. We will also have a bias vector with a component for each output channel. \n",
    "\n",
    "A big advantage of sharing weights and biases is that it greatly reduces the number of parameters involved in a convolutional network. For each feature map we need 25=5×5 shared weights, plus a single shared bias. So each feature map requires 26 parameters. If we have 32 feature maps that's a total of 32×26=832 parameters defining the convolutional layer. By comparison, suppose we had a fully connected first layer, with 784=28×28 input neurons, and a relatively modest 32 hidden neurons, as we used in many of the examples earlier in the book. That's a total of 784×32 weights, plus an extra 32 biases, for a total of 25,088 parameters. In other words, the fully-connected layer would have more than 30 times as many parameters as the convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then convolve x_image with the weight tensor, add the bias, apply the ReLU function, and finally max pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build a deep network, we stack several layers of this type. The second layer will have 64 features for each 5x5 patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "images/cnn.jpeg",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image('images/cnn.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, a fully connected layer that is able to combine previous layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to mention is regularization we applied dropout to prevent overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last stage compute probability and assign to each example a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.140625\n",
      "step 1000, training accuracy 0.96875\n",
      "step 2000, training accuracy 0.984375\n",
      "step 3000, training accuracy 1\n",
      "step 4000, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 6000, training accuracy 0.96875\n",
      "step 7000, training accuracy 1\n",
      "step 8000, training accuracy 0.984375\n",
      "step 9000, training accuracy 1\n",
      "step 10000, training accuracy 0.984375\n",
      "step 11000, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 13000, training accuracy 1\n",
      "step 14000, training accuracy 1\n",
      "step 15000, training accuracy 1\n",
      "step 16000, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 19000, training accuracy 1\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(20000):\n",
    "    batch = mnist.train.next_batch(64)\n",
    "    if i%1000 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch[0], \n",
    "                                                  y_: batch[1], \n",
    "                                                  keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.9924\n"
     ]
    }
   ],
   "source": [
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: mnist.test.images, \n",
    "                                                  y_: mnist.test.labels, \n",
    "                                                  keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Additional resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [TensorFlow playground](http://playground.tensorflow.org/)\n",
    "* [TensorFlow examples](https://github.com/aymericdamien/TensorFlow-Examples)\n",
    "* [Colah's Blog](http://colah.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "Visit [www.add-for.com](<http://www.add-for.com/IT>) for more tutorials and updates.\n",
    "\n",
    "This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tfkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
